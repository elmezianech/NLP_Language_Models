{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2124baec-d62e-4bd8-b2bd-7aa8495491b3",
   "metadata": {},
   "source": [
    "# Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa20767-a7fa-4882-a0d1-bde0589e6200",
   "metadata": {},
   "source": [
    "##### Objective : The main purpose behind this lab is to get familiar with NLP language models using Pytorch library.\r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09e5a3-8380-4399-bbe7-0fb70465bf49",
   "metadata": {},
   "source": [
    "## Part 3 : BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa817a6-e646-4f6d-bfbc-cd1a3b3a058b",
   "metadata": {},
   "source": [
    "### Step 1: Establish the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13c0f87-9987-4a6f-80b0-249abbd7d366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.36.2)\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6be0c411-7aec-4c2c-a55b-40aa0abca107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import Trainer, TrainingArguments, default_data_collator\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad4a72c-5ff5-4d2c-b2c8-a21ce21d9537",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388413e4-6cb6-4985-9b0d-027092042d6f",
   "metadata": {},
   "source": [
    "DataSet : https://nijianmo.github.io/amazon/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "646c4071-5346-49e7-ac58-1f5d611e3424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 reviewText  overall\n",
       "0  Great product and price!      5.0\n",
       "1  Great product and price!      5.0\n",
       "2  Great product and price!      5.0\n",
       "3  Great product and price!      5.0\n",
       "4  Great product and price!      5.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to parse the JSON file\n",
    "def parse(path):\n",
    "    with gzip.open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "# Function to get the DataFrame\n",
    "def get_df(path):\n",
    "    data = []\n",
    "    for review in parse(path):\n",
    "        data.append(review)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load the entire Amazon Fashion dataset\n",
    "df = get_df('AMAZON_FASHION_5.json.gz')\n",
    "\n",
    "# Keep only necessary columns and drop rows with missing values\n",
    "df = df[['reviewText', 'overall']].dropna()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7474b68d-e21c-4d16-8788-16c91daa8f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great product and price!</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Waaay too small. Will use for futur children!</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stays vibrant after many washes</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>My son really likes the pink. Ones which I was...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Waaay too small. Will use for future child.</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>I wear these everyday to work, the gym, etc.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3115</th>\n",
       "      <td>Very comfortable and fits perfectly</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3116</th>\n",
       "      <td>Super.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3117</th>\n",
       "      <td>Largely my fault for not reading carefully, bu...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3130</th>\n",
       "      <td>Size, colour and print all above average but d...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>440 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviewText  overall\n",
       "0                              Great product and price!      5.0\n",
       "5         Waaay too small. Will use for futur children!      3.0\n",
       "6                       Stays vibrant after many washes      5.0\n",
       "8     My son really likes the pink. Ones which I was...      5.0\n",
       "9           Waaay too small. Will use for future child.      3.0\n",
       "...                                                 ...      ...\n",
       "2380       I wear these everyday to work, the gym, etc.      5.0\n",
       "3115                Very comfortable and fits perfectly      5.0\n",
       "3116                                             Super.      5.0\n",
       "3117  Largely my fault for not reading carefully, bu...      4.0\n",
       "3130  Size, colour and print all above average but d...      4.0\n",
       "\n",
       "[440 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "73d55585-a6e6-4a9c-b15f-27288243e7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    293\n",
       "4.0     66\n",
       "3.0     49\n",
       "1.0     17\n",
       "2.0     15\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "caaa4602-5b44-4e4e-8a6a-cb25fafd8385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>great product price</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>waaay small use futur children</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stays vibrant many washes</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>son really likes pink ones nervous</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>waaay small use future child</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>wear everyday work gym etc</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3115</th>\n",
       "      <td>comfortable fits perfectly</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3116</th>\n",
       "      <td>super</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3117</th>\n",
       "      <td>largely fault reading carefully high synthetic...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3130</th>\n",
       "      <td>size colour print average wash great</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>440 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviewText  overall\n",
       "0                                   great product price      5.0\n",
       "5                        waaay small use futur children      3.0\n",
       "6                             stays vibrant many washes      5.0\n",
       "8                    son really likes pink ones nervous      5.0\n",
       "9                          waaay small use future child      3.0\n",
       "...                                                 ...      ...\n",
       "2380                         wear everyday work gym etc      5.0\n",
       "3115                         comfortable fits perfectly      5.0\n",
       "3116                                              super      5.0\n",
       "3117  largely fault reading carefully high synthetic...      4.0\n",
       "3130               size colour print average wash great      4.0\n",
       "\n",
       "[440 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters, punctuation, and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "df['reviewText'] = df['reviewText'].apply(preprocess_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2a0305a7-2945-4bbe-aaa9-e91b41801c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 352\n",
      "Testing set size: 88\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the size of training and testing sets\n",
    "print(f\"Training set size: {train_df.shape[0]}\")\n",
    "print(f\"Testing set size: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc14055-394d-43aa-b6cc-6e7a74061f41",
   "metadata": {},
   "source": [
    "### Step 3: Fine-tune and Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2319fe0-f7e7-423d-9ba6-751245481d33",
   "metadata": {},
   "source": [
    "#### Tokenization and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "289dc1fc-060d-4c07-a354-d16a63410e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_DISABLE_TFDS_SYMLINKS'] = \"1\"\n",
    "\n",
    "# Tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "# Dataset class for BERT\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, tokenizer, max_length):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        label = self.labels[item] - 1\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'review_text': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = ReviewDataset(\n",
    "    reviews=train_df.reviewText.to_numpy(),\n",
    "    labels=train_df.overall.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=160\n",
    ")\n",
    "\n",
    "test_dataset = ReviewDataset(\n",
    "    reviews=test_df.reviewText.to_numpy(),\n",
    "    labels=test_df.overall.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=160\n",
    ")\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b88bfd1-cec6-4197-9887-7c9b726302d6",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "96712c1e-9cc5-40a4-99b0-800062800724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 40:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=110, training_loss=1.342120326649059, metrics={'train_runtime': 2483.2417, 'train_samples_per_second': 0.709, 'train_steps_per_second': 0.044, 'total_flos': 144714978355200.0, 'train_loss': 1.342120326649059, 'epoch': 5.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=1000,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47adf580-3f9f-4e85-9bb8-59786dabc9e3",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e792e301-2139-4f97-ae48-64e53f221b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9107b2dcf3244d28bfcaa6c0c65fd1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c4d3ed50184c3097ffd521708b59d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d0a20f58c64fcca2642db2d966dc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c93b4a1e5e49489fe06c4bb787f915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7cf50efae543fc8b1d2b5fd1d88836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079fef75892b42188ecd4704a27e2bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6477272727272727\n",
      "F1 Score: 0.5092476489028214\n",
      "BLEU Score: 0.37511372399251636\n",
      "BERTScore: {'precision': tensor(0.9208), 'recall': tensor(0.8859), 'f1': tensor(0.9030)}\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1) + 1  # Adding 1 to adjust labels\n",
    "    labels = p.label_ids + 1  # Adding 1 to adjust labels\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall, \"confusion_matrix\": cm}\n",
    "\n",
    "# Predict and compute metrics\n",
    "predictions, labels, _ = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "acc = accuracy_score(labels, preds)\n",
    "f1 = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "# BLEU requires different input formats\n",
    "bleu_score = sentence_bleu([labels.tolist()], preds.tolist())\n",
    "\n",
    "# BERTScore requires different input formats\n",
    "P, R, F1 = score([\" \".join(str(p) for p in preds)], [\" \".join(str(l) for l in labels)], lang='en')\n",
    "bertscore_results = {\"precision\": P.mean(), \"recall\": R.mean(), \"f1\": F1.mean()}\n",
    "\n",
    "print(f'Accuracy: {acc}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'BLEU Score: {bleu_score}')\n",
    "print(f'BERTScore: {bertscore_results}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3968ce-02f5-4a40-a383-f17ad22b5703",
   "metadata": {},
   "source": [
    "#### Step 5: Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f6f55-14ab-4d81-b7fa-62f2262e7900",
   "metadata": {},
   "source": [
    "Fine-tuning a pre-trained BERT model for text classification tasks leverages the extensive contextual understanding that BERT has developed through its pre-training on a large corpus of text. This approach can significantly improve performance, particularly when compared to traditional machine learning models that rely on less sophisticated feature extraction methods. However, careful attention must be paid to the imbalanced nature of the dataset, and techniques such as class weighting or oversampling may be necessary to ensure the model performs well across all classes. Additionally, the choice of hyperparameters and the size of the dataset for fine-tuning are critical factors that influence the model's effectiveness.\r\n",
    "\r\n",
    "This method provides a powerful tool for handling complex text classification tasks, even with challenging datasets, by utilizing the state-of-the-art capabilities of BERT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
